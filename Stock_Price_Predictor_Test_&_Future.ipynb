{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock Price Predictor Test & Future.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1Xl9qCzQsUo"
      },
      "source": [
        "# Enter Stock Name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LAWLca7Qm3e",
        "outputId": "abb182cd-5d6a-43ad-8b69-b3ba195affa3"
      },
      "source": [
        "stock = input('Please enter Ticker Symbol(e.g. AAPL) & Press Enter: ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter Ticker Symbol(e.g. AAPL) & Press Enter: FORD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z33xOjedQ3Ug"
      },
      "source": [
        "# Getting Data & Calculating Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ0G3FG5Q2sA",
        "outputId": "4aafdf1c-3132-4897-db86-87964e3cd556"
      },
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests \n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        " \n",
        "# get data / handle time / convert to numpy array\n",
        "now = int(time.time()) \n",
        "now =str(now)\n",
        "days_delta = int(time.time()) - 12500000  #102 days of stock onfo\n",
        "days =str(days_delta)\n",
        "r = requests.get('https://finnhub.io/api/v1/stock/candle?symbol='+stock+'&from='+days+'&to='+now+'&resolution=D&token=c36j4jqad3ifoi8hsu50')\n",
        "j = r.json() \n",
        "df = pd.DataFrame.from_dict(j)\n",
        "df.t = (pd.to_datetime(df['t'],unit='s'))\n",
        "df_close = df[['t','c']]\n",
        "all_close = df_close['c'].values.astype(float)\n",
        " \n",
        "# libraries\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "# model building\n",
        "train=all_close[:85].reshape(-1, 1)\n",
        "test=all_close[85:].reshape(-1, 1)\n",
        "scaler=MinMaxScaler()\n",
        "scaled_train=scaler.fit_transform(train)\n",
        "scaled_test=scaler.transform(test)\n",
        " \n",
        "n_input=84\n",
        "n_features=1\n",
        " \n",
        "train_generator=TimeseriesGenerator(scaled_train,\n",
        "                                     scaled_train,\n",
        "                                      n_input,\n",
        "                                      batch_size=1)\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping()\n",
        " \n",
        "custom_early_stopping = EarlyStopping(   \n",
        "    monitor='loss', \n",
        "    patience=70, \n",
        "    min_delta=0.001, \n",
        "    mode='min' # was max\n",
        ")\n",
        "model=Sequential()\n",
        "model.add(LSTM(100,activation='relu',input_shape=(n_input,n_features),return_sequences=True))\n",
        "model.add(LSTM(50,activation='relu',return_sequences=True))\n",
        "model.add(LSTM(10,activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam',loss='mse')\n",
        " \n",
        "##\n",
        " \n",
        "model.fit(train_generator,epochs=140, callbacks=[custom_early_stopping])\n",
        " \n",
        "test_predictions = []\n",
        "#Select last n_input values from the train data\n",
        "first_eval_batch = scaled_train[-n_input:]\n",
        "#reshape the data into LSTM required (#batch,#timesteps,#features)\n",
        "current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
        "for i in range(len(test)):\n",
        "# get prediction, grab the exact number using the [0]\n",
        "  pred = model.predict(current_batch)[0]\n",
        "# Add this prediction to the list\n",
        "  test_predictions.append(pred)\n",
        "# The most critical part, update the (#batch,#timesteps,#features\n",
        "# using np.append(\n",
        "# current_batch[:        ,1:   ,:] ---------> read this as\n",
        "# current_batch[no_change,1:end,no_change]\n",
        "# (Do note the second part has the timesteps)\n",
        "# [[pred]] need the double brackets as current_batch is a 3D array\n",
        "# axis=1, remember we need to add to the second part i.e. 1st axis\n",
        "  current_batch = np.append(current_batch[:,1:,:],[[pred]],axis=1)\n",
        " \n",
        " \n",
        "actual_predictions = scaler.inverse_transform(test_predictions)\n",
        "rows =list(range(85, 100))\n",
        "pred_df = pd.DataFrame(data=actual_predictions,index=rows)\n",
        " \n",
        "future_train = all_close[15:].reshape(-1,1)\n",
        "scaled_future_train=scaler.fit_transform(future_train)\n",
        "n_input=84\n",
        "n_features=1\n",
        " \n",
        "future_generator=TimeseriesGenerator(scaled_future_train,\n",
        "                                     scaled_future_train,\n",
        "                                      n_input,\n",
        "                                      batch_size=1)\n",
        "custom_early_stopping = EarlyStopping(   \n",
        "    monitor='loss', \n",
        "    patience=70, \n",
        "    min_delta=0.001, \n",
        "    mode='min'\n",
        ")\n",
        " \n",
        "model=Sequential()\n",
        "model.add(LSTM(100,activation='relu',input_shape=(n_input,n_features),return_sequences=True))\n",
        "model.add(LSTM(50,activation='relu',return_sequences=True))\n",
        "model.add(LSTM(10,activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam',loss='mse')\n",
        " \n",
        " \n",
        "## custom stop was here\n",
        " \n",
        "model.fit(future_generator,epochs=140, callbacks=[custom_early_stopping])\n",
        " \n",
        "future_predictions = []\n",
        "#Select last n_input values from the train data\n",
        "first_eval_batch = scaled_future_train[-n_input:]\n",
        "#reshape the data into LSTM required (#batch,#timesteps,#features)\n",
        "current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
        "for i in range(len(test)):\n",
        "# get prediction, grab the exact number using the [0]\n",
        "  pred = model.predict(current_batch)[0]\n",
        "# Add this prediction to the list\n",
        "  future_predictions.append(pred)\n",
        "# The most critical part, update the (#batch,#timesteps,#features\n",
        "# using np.append(\n",
        "# current_batch[:        ,1:   ,:] ---------> read this as\n",
        "# current_batch[no_change,1:end,no_change]\n",
        "# (Do note the second part has the timesteps)\n",
        "# [[pred]] need the double brackets as current_batch is a 3D array\n",
        "# axis=1, remember we need to add to the second part i.e. 1st axis\n",
        "  current_batch = np.append(current_batch[:,1:,:],[[pred]],axis=1)\n",
        " \n",
        " \n",
        "future_actual_predictions = scaler.inverse_transform(future_predictions)\n",
        "rows =list(range(100, 115))\n",
        "future_pred_df = pd.DataFrame(data=future_actual_predictions,index=rows)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/140\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2666\n",
            "Epoch 2/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.2383\n",
            "Epoch 3/140\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.2215\n",
            "Epoch 4/140\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.2024\n",
            "Epoch 5/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1814\n",
            "Epoch 6/140\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.1588\n",
            "Epoch 7/140\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.1348\n",
            "Epoch 8/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1088\n",
            "Epoch 9/140\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0807\n",
            "Epoch 10/140\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0514\n",
            "Epoch 11/140\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0227\n",
            "Epoch 12/140\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0018\n",
            "Epoch 13/140\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0134\n",
            "Epoch 14/140\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0399\n",
            "Epoch 15/140\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0275\n",
            "Epoch 16/140\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0097\n",
            "Epoch 17/140\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0010\n",
            "Epoch 18/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 4.7542e-04\n",
            "Epoch 19/140\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0036\n",
            "Epoch 20/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0070\n",
            "Epoch 21/140\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0095\n",
            "Epoch 22/140\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0107\n",
            "Epoch 23/140\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0106\n",
            "Epoch 24/140\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0094\n",
            "Epoch 25/140\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0075\n",
            "Epoch 26/140\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0053\n",
            "Epoch 27/140\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0031\n",
            "Epoch 28/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0013\n",
            "Epoch 29/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.9614e-04\n",
            "Epoch 30/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 7.2094e-05\n",
            "Epoch 31/140\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 8.2095e-04\n",
            "Epoch 32/140\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0019\n",
            "Epoch 33/140\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0028\n",
            "Epoch 34/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0029\n",
            "Epoch 35/140\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0025\n",
            "Epoch 36/140\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0016\n",
            "Epoch 37/140\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 7.5246e-04\n",
            "Epoch 38/140\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.7865e-04\n",
            "Epoch 39/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 7.7547e-08\n",
            "Epoch 40/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.5063e-04\n",
            "Epoch 41/140\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 4.7387e-04\n",
            "Epoch 42/140\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 7.9643e-04\n",
            "Epoch 43/140\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 9.9409e-04\n",
            "Epoch 44/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0010\n",
            "Epoch 45/140\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 8.7664e-04\n",
            "Epoch 46/140\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 6.3076e-04\n",
            "Epoch 47/140\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 3.5893e-04\n",
            "Epoch 48/140\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.3590e-04\n",
            "Epoch 49/140\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.5224e-05\n",
            "Epoch 50/140\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.3019e-05\n",
            "Epoch 51/140\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.0329e-04\n",
            "Epoch 52/140\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 2.2911e-04\n",
            "Epoch 53/140\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 3.2685e-04\n",
            "Epoch 54/140\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 3.5371e-04\n",
            "Epoch 55/140\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 3.0416e-04\n",
            "Epoch 56/140\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 2.0562e-04\n",
            "Epoch 57/140\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.0077e-04\n",
            "Epoch 58/140\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 2.6453e-05\n",
            "Epoch 59/140\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 5.7156e-08\n",
            "Epoch 60/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.7429e-05\n",
            "Epoch 61/140\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 5.9610e-05\n",
            "Epoch 62/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.0313e-04\n",
            "Epoch 63/140\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.2915e-04\n",
            "Epoch 64/140\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.2889e-04\n",
            "Epoch 65/140\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.0465e-04\n",
            "Epoch 66/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 6.7094e-05\n",
            "Epoch 67/140\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 3.0280e-05\n",
            "Epoch 68/140\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 6.1470e-06\n",
            "Epoch 69/140\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 2.9705e-07\n",
            "Epoch 70/140\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.0543e-05\n",
            "Epoch 71/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2.8633e-05\n",
            "Epoch 72/140\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 4.4397e-05\n",
            "Epoch 73/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 5.0332e-05\n",
            "Epoch 74/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 4.4488e-05\n",
            "Epoch 75/140\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 3.0439e-05\n",
            "Epoch 76/140\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 1.4755e-05\n",
            "Epoch 77/140\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 3.5676e-06\n",
            "Epoch 78/140\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 7.4593e-09\n",
            "Epoch 79/140\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 3.4457e-06\n",
            "Epoch 80/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.0508e-05\n",
            "Epoch 81/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.7010e-05\n",
            "Epoch 82/140\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 1.9828e-05\n",
            "Epoch 83/140\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.7985e-05\n",
            "Epoch 84/140\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 1.2671e-05\n",
            "Epoch 85/140\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 6.3845e-06\n",
            "Epoch 86/140\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 1.6666e-06\n",
            "Epoch 87/140\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 2.0635e-10\n",
            "Epoch 88/140\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.3154e-06\n",
            "Epoch 1/140\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2332\n",
            "Epoch 2/140\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.2117\n",
            "Epoch 3/140\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.1950\n",
            "Epoch 4/140\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.1793\n",
            "Epoch 5/140\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.1648\n",
            "Epoch 6/140\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.1494\n",
            "Epoch 7/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1361\n",
            "Epoch 8/140\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1217\n",
            "Epoch 9/140\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.1055\n",
            "Epoch 10/140\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0869\n",
            "Epoch 11/140\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0663\n",
            "Epoch 12/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0442\n",
            "Epoch 13/140\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0217\n",
            "Epoch 14/140\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0034\n",
            "Epoch 15/140\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0056\n",
            "Epoch 16/140\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0319\n",
            "Epoch 17/140\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0234\n",
            "Epoch 18/140\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0080\n",
            "Epoch 19/140\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 7.8154e-04\n",
            "Epoch 20/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 4.4554e-04\n",
            "Epoch 21/140\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0031\n",
            "Epoch 22/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0060\n",
            "Epoch 23/140\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0081\n",
            "Epoch 24/140\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0089\n",
            "Epoch 25/140\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0086\n",
            "Epoch 26/140\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0074\n",
            "Epoch 27/140\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0056\n",
            "Epoch 28/140\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0036\n",
            "Epoch 29/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0019\n",
            "Epoch 30/140\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 6.0579e-04\n",
            "Epoch 31/140\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 3.1728e-05\n",
            "Epoch 32/140\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.4923e-04\n",
            "Epoch 33/140\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 7.1471e-04\n",
            "Epoch 34/140\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0014\n",
            "Epoch 35/140\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0017\n",
            "Epoch 36/140\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0017\n",
            "Epoch 37/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0012\n",
            "Epoch 38/140\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 6.4279e-04\n",
            "Epoch 39/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.9363e-04\n",
            "Epoch 40/140\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 4.7471e-06\n",
            "Epoch 41/140\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 7.4237e-05\n",
            "Epoch 42/140\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.8999e-04\n",
            "Epoch 43/140\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 5.1086e-04\n",
            "Epoch 44/140\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 6.2916e-04\n",
            "Epoch 45/140\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 6.1067e-04\n",
            "Epoch 46/140\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 4.8023e-04\n",
            "Epoch 47/140\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 2.9658e-04\n",
            "Epoch 48/140\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.2641e-04\n",
            "Epoch 49/140\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 2.1626e-05\n",
            "Epoch 50/140\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 3.4258e-06\n",
            "Epoch 51/140\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 5.7421e-05\n",
            "Epoch 52/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.4275e-04\n",
            "Epoch 53/140\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 2.1201e-04\n",
            "Epoch 54/140\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 2.3280e-04\n",
            "Epoch 55/140\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.9982e-04\n",
            "Epoch 56/140\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 1.3255e-04\n",
            "Epoch 57/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 6.1772e-05\n",
            "Epoch 58/140\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.3766e-05\n",
            "Epoch 59/140\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 1.7133e-07\n",
            "Epoch 60/140\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 1.6546e-05\n",
            "Epoch 61/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 4.7783e-05\n",
            "Epoch 62/140\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 7.6287e-05\n",
            "Epoch 63/140\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 8.9264e-05\n",
            "Epoch 64/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 8.2543e-05\n",
            "Epoch 65/140\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 6.0530e-05\n",
            "Epoch 66/140\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 3.3051e-05\n",
            "Epoch 67/140\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.0677e-05\n",
            "Epoch 68/140\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 4.1251e-07\n",
            "Epoch 69/140\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 3.3168e-06\n",
            "Epoch 70/140\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.4828e-05\n",
            "Epoch 71/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2.7468e-05\n",
            "Epoch 72/140\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 3.4542e-05\n",
            "Epoch 73/140\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 3.3029e-05\n",
            "Epoch 74/140\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 2.4362e-05\n",
            "Epoch 75/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.3009e-05\n",
            "Epoch 76/140\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 3.8899e-06\n",
            "Epoch 77/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 6.7970e-08\n",
            "Epoch 78/140\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.7273e-06\n",
            "Epoch 79/140\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 6.6433e-06\n",
            "Epoch 80/140\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 1.1619e-05\n",
            "Epoch 81/140\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 1.4059e-05\n",
            "Epoch 82/140\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 1.2991e-05\n",
            "Epoch 83/140\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 9.2024e-06\n",
            "Epoch 84/140\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 4.5932e-06\n",
            "Epoch 85/140\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.1434e-06\n",
            "Epoch 86/140\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.5546e-09\n",
            "Epoch 87/140\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.0826e-06\n",
            "Epoch 88/140\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 3.2945e-06\n",
            "Epoch 89/140\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 5.2091e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9aVIChWRfjJ"
      },
      "source": [
        "# Visualize The Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "kcNGwvUrRlqP",
        "outputId": "521ff201-1c2a-4951-f30d-c089cb2e4620"
      },
      "source": [
        "fig = px.line(df,x=df.index, y='c',template='plotly_dark')\n",
        "fig.add_trace(go.Scatter(x=pred_df.index, y=pred_df[0], mode=\"lines\",name='Test_Prediction'))\n",
        "fig.add_trace(go.Scatter(x=future_pred_df.index, y=future_pred_df[0], mode=\"lines\",name='Future_Prediction'))\n",
        "fig.update_layout(title=stock+' Stock Price LSTM prediction & performance for 15 test days')\n",
        "fig.update_yaxes(title='$')\n",
        "fig.update_xaxes(title='Days')\n",
        "#fig.update_layout(xaxis_rangeslider_visible=True)\n",
        "fig.show()\n",
        "#fig.to_html('pred.html')\n",
        "fig.write_html(stock+' Pred.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"1a5e8e2a-7faa-456c-8854-172f3ad84ddf\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"1a5e8e2a-7faa-456c-8854-172f3ad84ddf\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '1a5e8e2a-7faa-456c-8854-172f3ad84ddf',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"index=%{x}<br>c=%{y}\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], \"xaxis\": \"x\", \"y\": [3.05, 2.96, 2.85, 2.7, 2.96, 2.97, 2.92, 2.83, 2.78, 2.85, 2.82, 2.92, 2.93, 3.02, 3.0, 2.95, 2.76, 2.78, 2.73, 2.75, 2.75, 2.77, 2.66, 2.7, 2.7081, 2.7, 2.71, 2.68, 2.71, 2.71, 2.7, 2.74, 2.54, 2.5, 2.3, 2.43, 2.36, 2.3, 2.28, 2.21, 2.21, 2.28, 2.44, 2.47, 2.83, 2.65, 2.53, 2.4, 2.44, 2.83, 2.85, 2.71, 2.69, 2.65, 2.71, 2.88, 2.99, 2.89, 2.84, 2.93, 2.82, 2.73, 3.16, 3.21, 3.0, 2.87, 2.88, 3.01, 3.04, 3.07, 2.98, 2.96, 2.96, 2.98, 3.13, 3.07, 2.875, 2.79, 2.77, 2.87, 2.74, 2.67, 2.6, 2.535, 2.74, 2.66, 2.73, 2.66, 2.58, 2.59, 2.62, 2.56, 2.78, 2.62, 2.63, 2.61, 2.57, 2.68, 2.73, 2.68], \"yaxis\": \"y\"}, {\"mode\": \"lines\", \"name\": \"Test_Prediction\", \"type\": \"scatter\", \"x\": [85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], \"y\": [2.7328207111358642, 2.723033628463745, 2.713215789794922, 2.703670731782913, 2.69455947637558, 2.685951313972473, 2.6778591787815094, 2.6702636098861694, 2.6631289935112, 2.656413606405258, 2.650075933933258, 2.6440771734714508, 2.638383708000183, 2.6329663610458374, 2.6278009927272796]}, {\"mode\": \"lines\", \"name\": \"Future_Prediction\", \"type\": \"scatter\", \"x\": [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114], \"y\": [2.6770264720916748, 2.6725418186187744, 2.6688354229927063, 2.665782026052475, 2.6632621800899505, 2.6611695504188537, 2.659412703514099, 2.6579160606861114, 2.6566184079647064, 2.6554716742038726, 2.654438546895981, 2.6534907734394073, 2.652607581615448, 2.651773533821106, 2.6509778714179992]}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#f2f5fa\"}, \"error_y\": {\"color\": \"#f2f5fa\"}, \"marker\": {\"line\": {\"color\": \"rgb(17,17,17)\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"rgb(17,17,17)\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#A2B1C6\", \"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"minorgridcolor\": \"#506784\", \"startlinecolor\": \"#A2B1C6\"}, \"baxis\": {\"endlinecolor\": \"#A2B1C6\", \"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"minorgridcolor\": \"#506784\", \"startlinecolor\": \"#A2B1C6\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"line\": {\"color\": \"#283442\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"line\": {\"color\": \"#283442\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#506784\"}, \"line\": {\"color\": \"rgb(17,17,17)\"}}, \"header\": {\"fill\": {\"color\": \"#2a3f5f\"}, \"line\": {\"color\": \"rgb(17,17,17)\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#f2f5fa\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#f2f5fa\"}, \"geo\": {\"bgcolor\": \"rgb(17,17,17)\", \"lakecolor\": \"rgb(17,17,17)\", \"landcolor\": \"rgb(17,17,17)\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"#506784\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"dark\"}, \"paper_bgcolor\": \"rgb(17,17,17)\", \"plot_bgcolor\": \"rgb(17,17,17)\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"ticks\": \"\"}, \"bgcolor\": \"rgb(17,17,17)\", \"radialaxis\": {\"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"rgb(17,17,17)\", \"gridcolor\": \"#506784\", \"gridwidth\": 2, \"linecolor\": \"#506784\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"#C8D4E3\"}, \"yaxis\": {\"backgroundcolor\": \"rgb(17,17,17)\", \"gridcolor\": \"#506784\", \"gridwidth\": 2, \"linecolor\": \"#506784\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"#C8D4E3\"}, \"zaxis\": {\"backgroundcolor\": \"rgb(17,17,17)\", \"gridcolor\": \"#506784\", \"gridwidth\": 2, \"linecolor\": \"#506784\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"#C8D4E3\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#f2f5fa\"}}, \"sliderdefaults\": {\"bgcolor\": \"#C8D4E3\", \"bordercolor\": \"rgb(17,17,17)\", \"borderwidth\": 1, \"tickwidth\": 0}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"ticks\": \"\"}, \"bgcolor\": \"rgb(17,17,17)\", \"caxis\": {\"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"updatemenudefaults\": {\"bgcolor\": \"#506784\", \"borderwidth\": 0}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"#283442\", \"linecolor\": \"#506784\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"#283442\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"#283442\", \"linecolor\": \"#506784\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"#283442\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"FORD Stock Price LSTM prediction & performance for 15 test days\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Days\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"$\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1a5e8e2a-7faa-456c-8854-172f3ad84ddf');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}